<h2 style="text-align: center; margin-top: -150px; color: #3498db;">Research</h2>

<!-- Research interests section -->
<div style="padding-left: 5%; padding-right: 5%;">
    <div style="width: 100%; padding: 8px; margin-bottom: 20px; text-align: center; font-size: large;">
        Here's some information about the work I do, that I find particularly interesting.<br/>Send me a message if we have overlapping interests.
    </div>

    <div class="research_image">
        <img src="{{ site.baseurl }}/assets/img/mario.gif" class="profile-image">
    </div>

    <!-- Grid container for 2x2 layout -->
    <div class="grid-container">
        <div class="research_box"><strong>ðŸ”„ Continual Learning</strong><br> Is there a <a href="https://link.springer.com/content/pdf/10.1007/978-3-319-15524-1.pdf">myth of the objective</a>? Does it even exist, or are we just chasing shadows? This leads onto the real question: how do we design an algorithm that will last for the next thousand years?
            <br><br>
            <a href="https://arxiv.org/abs/1909.08383">Continual Learning Survey</a><br>
            <a href="https://arxiv.org/abs/2302.00487">Another Continual Learning Survey</a> - Both of these surveys are great<br>
            <a href="https://www.youtube.com/watch?v=Ugp1osXLEd4">A Definition of Continual Learning</a> - a definition of continual DRL by one of my PhD supervisors.
        </div>

        <div class="research_box"><strong>ðŸ“œ Magnus FinisÂ </strong><br> Stop doing research for the sake of research. What was the end goal again? How do we get there? Well, here are a few interesting takes.
            <br><br>
            <a href="https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf">Can Machines Think?</a> - the man himself.<br>
            <a href="https://arxiv.org/abs/2208.11173">The Alberta Plan</a> - Richard Sutton's take.<br>
            <a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">Path to AMI</a> - Yann LeCun's take.<br>
            <a href="https://openreview.net/pdf?id=Sv7DazuCn8">The Big World Hypothesis</a> - short but sweet.
        </div>

        <div class="research_box"><strong>ðŸŽ¯ Zero-Shot Generalisation</strong><br>Show a monkey how to stack some bricks, and itâ€™ll ace the job no matter the shape, color, or pose. Teach the same trick to a DL model, and watch it have an aneurism when the colour of the brick changes. Personally, I'm not a fan of drowning models in heaps of training data, so how do we craft <strong>domain-invariant</strong> policies?
            <br><br>
            <a href="https://arxiv.org/abs/2111.09794">Zero-shot Generalisation in DRL Survey</a> - really nice survey on this topic by the lot in DARK labs, UCL.<br>
            <a href="https://arxiv.org/abs/2107.12808">Open-Ended Learning</a> - Even if you don't want to read this, at least get lost in its figures ðŸ¤¤.<br>
            <a href="https://ieeexplore.ieee.org/document/9847099?arnumber=9847099">Domain Generalisation Survey</a> - a broader look at the problem of domain invariance.<br>
            <a href="https://arxiv.org/abs/2202.00104">Generalisation in Cooperative MARL</a> - an interesting look into combinatorial generalisation.
        </div>

        <div class="research_box"><strong>ðŸ§  Representation Learning</strong><br> When you learn something new, what does it even mean to understand it? It's something we do quasi-proactively; half the time, it's like a mental magic trick we perform almost on autopilot. This notion of learning in DL is usually achieved with representation learning, where a model learns representations from raw data. This currently works pretty dang well in single-domain supervised learning tasks, but how can we create representations that are more representative of the mess of a world we live in?
            <br><br>
            <a href="https://arxiv.org/abs/2301.08243">I-JEPA</a> - can you even talk about representation learning without mentioning Yann's work?<br>
            <a href="https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/">V-JEPA</a> - V0.2 of Yann's vision.<br>
            <a href="https://arxiv.org/pdf/1206.5538">Representation Learning Review</a> - old but gold.<br>
            <a href="https://ieeexplore.ieee.org/document/8715409">Multimodal Representation Learning</a> - what it says on the tin.
        </div>
    </div>
</div>

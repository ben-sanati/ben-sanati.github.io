<h2 style="text-align: center; margin-top: -150px;">Research</h2>

<!-- Research interests section -->
<style>
    .research_box {
        padding: 15px;
        background-color: #1a1a1a;
        border: 2px solid #7603d4;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);
        transition: all 0.3s ease;
        color: #fff;
        font-family: 'Courier New', Courier, monospace;
    }

    .research_box:hover {
        border-color: #ff00e0;
        background-color: #262626;
        transform: scale(1.02);
    }

    /* Grid container for 2x2 layout */
    .grid-container {
        display: grid;
        grid-template-columns: repeat(2, 1fr);
        gap: 20px; /* Spacing between boxes */
        margin-top: 20px;
    }

    /* Ensure full width on smaller screens */
    @media (max-width: 768px) {
        .grid-container {
            grid-template-columns: 1fr;
        }
    }
</style>

<div style="padding-left: 5%; padding-right: 5%;">
    <div style="width: 100%; padding: 8px; margin-bottom: 20px; text-align: center; font-size: large;">
        Some areas I'm currently excited about. If you want to chat about research or are interested in interning at MSR, feel free to reach out over email :)
    </div>

    <!-- Grid container for 2x2 layout -->
    <div class="grid-container">
        <div class="research_box"><strong>ðŸ”Ž Interpretability.</strong> I'm interested in <a href="https://arxiv.org/abs/2402.01761">rethinking interpretability</a> in the context of LLMs
            <br><br>
            <a href="https://www.nature.com/articles/s41467-023-43713-1">augmented imodels</a> - use LLMs to build a transparent model<br>
            <a href="http://proceedings.mlr.press/v119/rieger20a.html">explanation penalization</a> - regularize explanations to align models with prior knowledge<br>
            <a href="https://proceedings.neurips.cc/paper/2021/file/acaa23f71f963e96c8847585e71352d6-Paper.pdf">adaptive wavelet distillation</a> - replace neural nets with simple, performant wavelet models
        </div>

        <div class="research_box"><strong>ðŸš— LLM steering. </strong>Interpretability tools can provide ways to better guide and use LLMs (without needing gradients!)
            <br><br>
            <a href="https://arxiv.org/abs/2310.14034">tree prompting</a> - improve black-box few-shot text classification with decision trees<br>
            <a href="https://arxiv.org/abs/2311.02262">attention steering</a> / <a href="https://arxiv.org/abs/2409.10790">automatic attention steering</a> - guide LLMs by emphasizing specific input spans<br>
            <a href="https://arxiv.org/abs/2210.01848">interpretable autoprompting</a> - automatically find fluent natural-language prompts<br>
        </div>

        <div class="research_box"><strong>ðŸ§  Neuroscience. </strong> Since joining MSR, I've been focused on leveraging LLM interpretability to understand how the human brain represents language (using fMRI in collaboration with the <a href="https://www.cs.utexas.edu/~huth/index.html">Huth lab</a> at UT Austin).
            <br><br>
            <a href="https://arxiv.org/abs/2405.16714">qa embeddings</a> - build interpretable fMRI encoding models by asking yes/no questions to LLMs<br>
            <a href="https://arxiv.org/abs/2305.09863">summarize &amp; score explanations</a> - generate natural-language explanations of fMRI encoding models
        </div>

        <div class="research_box"><strong>ðŸ’Š Healthcare.Â </strong> I'm also actively working on how we can improve clinical decision instruments by using the information contained across various sources in the medical literature (in collaboration with <a href="https://profiles.ucsf.edu/aaron.kornblith">Aaron Kornblith</a> at UCSF and the MSR <a href="https://www.microsoft.com/en-us/research/group/real-world-evidence/">Health Futures team</a>).
            <br><br>
            <a href="https://arxiv.org/abs/2306.00024">clinical self-verification</a> - self-verification improves performance and interpretability of clinical information extraction<br>
            <a href="https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000076">clinical rule vetting</a> - stress testing a clinical decision instrument performance for intra-abdominal injury
        </div>
    </div>

    <div style="width: 100%; padding: 8px; margin-bottom: 20px; text-align: center; font-size: large;">
        Across these areas, I'm interested in decision trees and how we can build flexible but accurate transparent models. I put a lot of my code into the <a href="https://github.com/csinva/imodels">imodels</a> and <a href="https://github.com/csinva/imodelsx">imodelsX</a> packages.
    </div>
</div>

<hr>
